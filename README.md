# âš½ Football Analytics Lakehouse â€“ Endâ€‘toâ€‘End Data Engineering Project

This repository contains a **productionâ€‘shaped local lakehouse pipeline** built to simulate realâ€‘world data engineering workflows using Apache Spark and Prefect.

The project ingests raw football datasets, models them into analytical tables, and orchestrates the entire Bronze â†’ Silver â†’ Gold lifecycle with validation, retries, and monitoring.

This is not a tutorial project. It is designed to practice **engineering thinking**: contracts between layers, standalone jobs, deterministic outputs, and operational recovery.

---

## ğŸ— Architecture (High Level)

```
Raw CSV
   â†“
Bronze (Parquet, faithful ingestion)
   â†“
Silver (modeled dimensions)
   â†“
Gold (facts + KPIs)

Prefect
   â†“
Spark Jobs (Bronze â†’ Silver â†’ Gold)
```

Each layer is implemented as an **independent Spark job**. Prefect orchestrates execution order, retries failures, and surfaces pipeline state.

---

## ğŸ§° Tech Stack

- Python 3
- Apache Spark (PySpark)
- Parquet
- Prefect (localâ€‘first orchestration)
- Linux / WSL

---

## ğŸ“‚ Project Structure

```
football-lakehouse/
â”‚
â”œâ”€â”€ raw/                     # Original CSV datasets
â”œâ”€â”€ bronze/                  # Raw â†’ Parquet landing zone
â”œâ”€â”€ silver/                  # Modeled dimensions
â”œâ”€â”€ gold/                    # Facts + KPI tables
â”‚
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ bronze_ingest.py
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ silver_transform.py
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â””â”€â”€ gold_fact_player_match.py
â”‚   â””â”€â”€ main_pipeline_flow.py
â”‚
â””â”€â”€ README.md
```

---

## âœ… Implemented Features

### Bronze Layer
- CSV ingestion via Spark
- Folderâ€‘perâ€‘table Parquet layout
- Row manifests
- Failâ€‘fast on empty tables

### Silver Layer
- Snake_case normalization
- Type casting
- Domain column selection
- Deduplication on business keys
- Basic data quality checks

### Gold Layer
- `fact_player_match` (player Ã— game spine)
- Player KPIs
- Playerâ€‘season KPIs
- Clubâ€‘season KPIs
- Metrics: matches, minutes, goals, assists, goal involvement, per90, cards
- Strict validation before write

### Orchestration (Prefect)
- Unified Bronze â†’ Silver â†’ Gold pipeline
- Standalone Spark jobs per layer
- Retry logic per task
- Flowâ€‘level monitoring
- Failâ€‘fast semantics

---

## â–¶ Running the Full Pipeline

From project root:

```bash
python pipelines/main_pipeline_flow.py
```

This executes:

```
Bronze â†’ Silver â†’ Gold
```

with retries and validation.

---

## â–¶ Running Individual Layers

Bronze:

```bash
python pipelines/bronze/bronze_ingest.py \
  --raw-path /home/gnana/football-lakehouse/raw \
  --bronze-path /home/gnana/football-lakehouse/bronze
```

Silver:

```bash
python pipelines/silver/silver_transform.py \
  --bronze-path /home/gnana/football-lakehouse/bronze \
  --silver-path /home/gnana/football-lakehouse/silver
```

Gold:

```bash
python pipelines/gold/gold_fact_player_match.py \
  --silver-path /home/gnana/football-lakehouse/silver \
  --gold-path /home/gnana/football-lakehouse/gold
```

---

## ğŸ§  Engineering Principles Practiced

- Layered lakehouse design (Bronze / Silver / Gold)
- Standalone, parameterized Spark jobs
- Idempotent writes
- Explicit validation and failure propagation
- Orchestration separated from business logic
- Rowâ€‘count verification
- Deterministic rebuilds

---

## ğŸš€ Roadmap

Next phases include:

- Backfills (partial reprocessing)
- Scheduling
- Streaming simulation
- Data quality contracts
- Feature tables + ML bridge

---

## ğŸ“Œ Why This Project

This project focuses on **systems thinking**, not just Spark usage:

- Designing contracts between layers
- Building recoverable pipelines
- Treating failures as firstâ€‘class citizens
- Operating data workflows like production systems

---

## Author

Built as part of a professional data engineering roadmap.

